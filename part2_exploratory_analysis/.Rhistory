#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
b_init=1
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:15)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
function_evaluation = function(x1, x2){
fun = function(t){
return(t*(3*x1^2-2*x1*x2+3*x2^2)-
log(x1)-log(x2)-
log(1-x1)-log(1-x2))
}
return(fun)
}
gradient_evaluation = function(x1, x2){
fun = function(t){
return(
c(t*(6*x1-2*x2)-1/x1-1/(x1-1),
t*(6*x2-2*x1)-1/x2-1/(x2-1))
)
}
return(fun)
}
hessian_evaluation = function(x1, x2){
fun = function(t){
return(matrix(
c(6*t+1/x1^2+1/(x1-1)^2,
-2*t,
-2*t,
6*t+1/x2^2+1/(x2-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
f = function_evaluation(1)
df = gradient_evaluation(1)
d2f = hessian_evaluation(1)
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
function_evaluation = function(x1, x2){
fun = function(t){
return(t*(3*x1^2-2*x1*x2+3*x2^2)-
log(x1)-log(x2)-
log(1-x1)-log(1-x2))
}
return(fun)
}
function_evaluation = function(x1, x2){
fun = function(t){
return(t*(3*x1^2-2*x1*x2+3*x2^2)-
log(x1)-log(x2)-
log(1-x1)-log(1-x2))
}
return(fun)
}
gradient_evaluation = function(x1, x2){
fun = function(t){
return(
c(t*(6*x1-2*x2)-1/x1-1/(x1-1),
t*(6*x2-2*x1)-1/x2-1/(x2-1))
)
}
return(fun)
}
hessian_evaluation = function(x1, x2){
fun = function(t){
return(matrix(
c(6*t+1/x1^2+1/(x1-1)^2,
-2*t,
-2*t,
6*t+1/x2^2+1/(x2-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
f = function_evaluation(1)
df = gradient_evaluation(1)
d2f = hessian_evaluation(1)
function_evaluation = function(x1, x2){
fun = function(t){
return(t*(3*x1^2-2*x1*x2+3*x2^2)-
log(x1)-log(x2)-
log(1-x1)-log(1-x2))
}
return(fun)
}
gradient_evaluation = function(x1, x2){
fun = function(t){
return(
c(t*(6*x1-2*x2)-1/x1-1/(x1-1),
t*(6*x2-2*x1)-1/x2-1/(x2-1))
)
}
return(fun)
}
hessian_evaluation = function(x1, x2){
fun = function(t){
return(matrix(
c(6*t+1/x1^2+1/(x1-1)^2,
-2*t,
-2*t,
6*t+1/x2^2+1/(x2-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
function_evaluation = function(t){
fun = function(x1,x2){
return(t*(3*x1^2-2*x1*x2+3*x2^2)-
log(x1)-log(x2)-
log(1-x1)-log(1-x2))
}
return(fun)
}
gradient_evaluation = function(t){
fun = function(x1,x2){
return(
c(t*(6*x1-2*x2)-1/x1-1/(x1-1),
t*(6*x2-2*x1)-1/x2-1/(x2-1))
)
}
return(fun)
}
hessian_evaluation = function(t){
fun = function(x1,x2){
return(matrix(
c(6*t+1/x1^2+1/(x1-1)^2,
-2*t,
-2*t,
6*t+1/x2^2+1/(x2-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
f = function_evaluation(1)
df = gradient_evaluation(1)
d2f = hessian_evaluation(1)
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
df(b_init)
function_evaluation = function(t){
fun = function(x){
return(t*(3*x[1]^2-2*x[1]*x[2]+3*x[2]^2)-
log(x[1])-log(x[2])-
log(1-x[1])-log(1-x[2]))
}
return(fun)
}
gradient_evaluation = function(t){
fun = function(x){
return(
c(t*(6*x[1]-2*x[2])-1/x[1]-1/(x[1]-1),
t*(6*x[2]-2*x[1])-1/x[2]-1/(x[2]-1))
)
}
return(fun)
}
hessian_evaluation = function(t){
fun = function(x){
return(matrix(
c(6*t+1/x[1]^2+1/(x[1]-1)^2,
-2*t,
-2*t,
6*t+1/x[2]^2+1/(x[2]-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
f = function_evaluation(1)
df = gradient_evaluation(1)
d2f = hessian_evaluation(1)
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:15)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b_init
b_new
b_new = list(b_new))
b_new = list(b_new)
b_new
b_new = list(b_new,2:100)
?list
b = as.list(1:100)
b[[1]] = b_new
b_new = b
b_new
b = as.list(1:100)
b[[1]]
b[[1]] = b_new
b[[1]]
b = list(1:100)
b = as.list(1:100)
b[[1]] = b_new
b_new = b
b[[1]]
b = as.list(1:100)
b[[1]][1] = b_new
b = list(b_new)
b[[2]] = 1
b
function_evaluation = function(t){
fun = function(x){
return(t*(3*x[1]^2-2*x[1]*x[2]+3*x[2]^2)-
log(x[1])-log(x[2])-
log(1-x[1])-log(1-x[2]))
}
return(fun)
}
gradient_evaluation = function(t){
fun = function(x){
return(
c(t*(6*x[1]-2*x[2])-1/x[1]-1/(x[1]-1),
t*(6*x[2]-2*x[1])-1/x[2]-1/(x[2]-1))
)
}
return(fun)
}
hessian_evaluation = function(t){
fun = function(x){
return(matrix(
c(6*t+1/x[1]^2+1/(x[1]-1)^2,
-2*t,
-2*t,
6*t+1/x[2]^2+1/(x[2]-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
### Part B
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
t = 1
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:15)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b_new
as.list(b_new)
b_new = list(b_new)
b_new
b_new[[2]] = c(1,1)
b_new
for(t in 2:100){
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=b[[t-1]]
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:5)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b[[t]] = b_new
}
b = list(b_new)
### Part C
for(t in 2:100){
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=b[[t-1]]
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:5)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b[[t]] = b_new
}
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
t = 1
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:14)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b = list(b_new)
### Part C
for(t in 2:100){
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=b[[t-1]]
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:4)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b[[t]] = b_new
}
plot(b)
sapply(b, function(x){x[1]})
sapply(b, function(x){x[2]})
?plot
plot(sapply(b, function(x){x[1]}), sapply(b, function(x){x[1]}), xlab = "x", ylab = "y", type = "l")
plot(c(0,1),c(0,1), xlab = "x1", ylab = "x2")
lines(sapply(b, function(x){x[1]}), sapply(b, function(x){x[1]}))
?lines
plot(sapply(b, function(x){x[1]}), sapply(b, function(x){x[1]}),xlim = c(0,2), ylim = c(0,2), xlab = "x1", ylab = "x2")
rect(0,0,1,1)
?lines
lines(c(0, .5), c(.5,.5))
lines(c(0, 1), c(.5,.5))
install.packages("network")
library(network)
help("network-package")
?"network"
setwd("Documents/Analytics 501 Fall 2015/50-percent-Chance-of-Awesome/part2_exploratory_analysis/")
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
train = sample(1:nrow(dataset),nrow(dataset)*2/3)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
tree_model = tree(traindata$WINNER~.,traindata[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")])
library(ISLR)
library(tree)
tree_model = tree(traindata$WINNER~.,traindata[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")])
plot(tree_model)
text(tree_model, pretty = 0)
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
dataset$WINNER = as.factor(dataset$WINNER)
train = sample(1:nrow(dataset),nrow(dataset)*2/3)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
tree_model = tree(traindata$WINNER~.,traindata[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")])
tree_predict = predict(tree_model, testdata, type = "class")
tree_predict
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
dataset$WINNER = as.factor(dataset$WINNER)
train = sample(1:nrow(dataset),nrow(dataset)*2/3)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")])
#check the performance of the model using test data
tree_predict = predict(tree_model, testdata, type = "class")
#using .5 as a standard split
print(table(tree_predict, testdata$WINNER))
print(mean(tree_predict==testdata$WINNER))
print(var(tree_predict==testdata$WINNER))
library(ISLR)
library(tree)
#load the dataset
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
dataset$WINNER = as.factor(dataset$WINNER)
for(k in 1:3){
train = sample(1:nrow(dataset),nrow(dataset)*2/3)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")])
#check the performance of the model using test data
pred = predict(tree_model, testdata, type = "class")
#using .5 as a standard split
print(paste0("accuracy: ", mean(pred==testdata$WINNER)),quote = FALSE)
#confint(tree_predict==testdata$class)
#how to get mean, variance, confidence interval???????????
#prune the tree to improve the performance
cv_tree = cv.tree(tree_model, FUN = prune.tree)
#see the cv_tree's attributes
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type = "b")
#we can see that when size = 3, the error is least
prune_model = prune.tree(tree_model, best = 3)
#show the tree structure after pruning
jpeg(paste0("hypothesis_testing/data_driving_predictive_models/DT Prune ", k, ".jpeg", sep = ""))
plot(prune_model)
text(prune_model, pretty = 0)
dev.off()
predict_after_prune = predict(prune_model, testdata)
confmatrix = (table(tree_predict_after_prune, testdata$WINNER))
print("confusion matrix",quote = FALSE)
print(confmatrix)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(tree_predict_after_prune)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
jpeg(paste0("hypothesis_testing/data_driving_predictive_models/DT ROC ", k, ".jpeg", sep = ""))
plot(myROC)
dev.off()
print(paste0("accuracy: ", mean(tree_predict_after_prune==testdata$WINNER)),quote = FALSE)
print(paste0("precision: ", precision),quote = FALSE)
print(paste0("Recall: ", Recall),quote = FALSE)
print(paste0("F-measure: ", 2*precision*Recall/(precision+Recall)),quote = FALSE)
}
source("hypothesis_testing/data_driving_predictive_models/DT.R")
source("hypothesis_testing/data_driving_predictive_models/DT.R")
source("hypothesis_testing/data_driving_predictive_models/DT.R")
train = sample(1:nrow(dataset),nrow(dataset)*2/3)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")])
