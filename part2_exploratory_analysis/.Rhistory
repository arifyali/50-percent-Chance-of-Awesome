nrow = 2,
ncol = 2
)
)
}
return(fun)
}
f = function_evaluation(1)
df = gradient_evaluation(1)
d2f = hessian_evaluation(1)
function_evaluation = function(x1, x2){
fun = function(t){
return(t*(3*x1^2-2*x1*x2+3*x2^2)-
log(x1)-log(x2)-
log(1-x1)-log(1-x2))
}
return(fun)
}
gradient_evaluation = function(x1, x2){
fun = function(t){
return(
c(t*(6*x1-2*x2)-1/x1-1/(x1-1),
t*(6*x2-2*x1)-1/x2-1/(x2-1))
)
}
return(fun)
}
hessian_evaluation = function(x1, x2){
fun = function(t){
return(matrix(
c(6*t+1/x1^2+1/(x1-1)^2,
-2*t,
-2*t,
6*t+1/x2^2+1/(x2-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
function_evaluation = function(t){
fun = function(x1,x2){
return(t*(3*x1^2-2*x1*x2+3*x2^2)-
log(x1)-log(x2)-
log(1-x1)-log(1-x2))
}
return(fun)
}
gradient_evaluation = function(t){
fun = function(x1,x2){
return(
c(t*(6*x1-2*x2)-1/x1-1/(x1-1),
t*(6*x2-2*x1)-1/x2-1/(x2-1))
)
}
return(fun)
}
hessian_evaluation = function(t){
fun = function(x1,x2){
return(matrix(
c(6*t+1/x1^2+1/(x1-1)^2,
-2*t,
-2*t,
6*t+1/x2^2+1/(x2-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
f = function_evaluation(1)
df = gradient_evaluation(1)
d2f = hessian_evaluation(1)
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
df(b_init)
function_evaluation = function(t){
fun = function(x){
return(t*(3*x[1]^2-2*x[1]*x[2]+3*x[2]^2)-
log(x[1])-log(x[2])-
log(1-x[1])-log(1-x[2]))
}
return(fun)
}
gradient_evaluation = function(t){
fun = function(x){
return(
c(t*(6*x[1]-2*x[2])-1/x[1]-1/(x[1]-1),
t*(6*x[2]-2*x[1])-1/x[2]-1/(x[2]-1))
)
}
return(fun)
}
hessian_evaluation = function(t){
fun = function(x){
return(matrix(
c(6*t+1/x[1]^2+1/(x[1]-1)^2,
-2*t,
-2*t,
6*t+1/x[2]^2+1/(x[2]-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
f = function_evaluation(1)
df = gradient_evaluation(1)
d2f = hessian_evaluation(1)
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:15)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b_init
b_new
b_new = list(b_new))
b_new = list(b_new)
b_new
b_new = list(b_new,2:100)
?list
b = as.list(1:100)
b[[1]] = b_new
b_new = b
b_new
b = as.list(1:100)
b[[1]]
b[[1]] = b_new
b[[1]]
b = list(1:100)
b = as.list(1:100)
b[[1]] = b_new
b_new = b
b[[1]]
b = as.list(1:100)
b[[1]][1] = b_new
b = list(b_new)
b[[2]] = 1
b
function_evaluation = function(t){
fun = function(x){
return(t*(3*x[1]^2-2*x[1]*x[2]+3*x[2]^2)-
log(x[1])-log(x[2])-
log(1-x[1])-log(1-x[2]))
}
return(fun)
}
gradient_evaluation = function(t){
fun = function(x){
return(
c(t*(6*x[1]-2*x[2])-1/x[1]-1/(x[1]-1),
t*(6*x[2]-2*x[1])-1/x[2]-1/(x[2]-1))
)
}
return(fun)
}
hessian_evaluation = function(t){
fun = function(x){
return(matrix(
c(6*t+1/x[1]^2+1/(x[1]-1)^2,
-2*t,
-2*t,
6*t+1/x[2]^2+1/(x[2]-1)^2),
nrow = 2,
ncol = 2
)
)
}
return(fun)
}
### Part B
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
t = 1
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:15)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b_new
as.list(b_new)
b_new = list(b_new)
b_new
b_new[[2]] = c(1,1)
b_new
for(t in 2:100){
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=b[[t-1]]
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:5)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b[[t]] = b_new
}
b = list(b_new)
### Part C
for(t in 2:100){
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=b[[t-1]]
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:5)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b[[t]] = b_new
}
backtracking = function(f,x0,df0,dx,a,b){
t = 1
while(f(x0 + t * dx) > f(x0) + (a*t*df0%*%dx)){
t = b*t
#print(c(t,f(x0 + t * dx),f(x0) + (a*t*df0%*%dx)));
}
return(x0+t*dx)
}
t = 1
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=c(.5,.5)
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:14)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b = list(b_new)
### Part C
for(t in 2:100){
f = function_evaluation(t)
df = gradient_evaluation(t)
d2f = hessian_evaluation(t)
b_init=b[[t-1]]
b_new=backtracking(f,b_init,df(b_init),solve(d2f(b_init),(-df(b_init))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c()
for(i in  1:4)
{
b_new=backtracking(f,b_new,df(b_new),solve(d2f(b_new),(-df(b_new))),0.25,0.5)
b_init = rbind(b_init, b_new)
residual_log = c(residual_log,
sqrt((b_new[1]-b_init[(nrow(b_init)-1),1])^2+(b_new[2]-b_init[(nrow(b_init)-1),2])^2))
}
b[[t]] = b_new
}
plot(b)
sapply(b, function(x){x[1]})
sapply(b, function(x){x[2]})
?plot
plot(sapply(b, function(x){x[1]}), sapply(b, function(x){x[1]}), xlab = "x", ylab = "y", type = "l")
plot(c(0,1),c(0,1), xlab = "x1", ylab = "x2")
lines(sapply(b, function(x){x[1]}), sapply(b, function(x){x[1]}))
?lines
plot(sapply(b, function(x){x[1]}), sapply(b, function(x){x[1]}),xlim = c(0,2), ylim = c(0,2), xlab = "x1", ylab = "x2")
rect(0,0,1,1)
?lines
lines(c(0, .5), c(.5,.5))
lines(c(0, 1), c(.5,.5))
install.packages("network")
library(network)
help("network-package")
?"network"
library(ISLR)
library(tree)
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
setwd("Documents/Analytics 501 Fall 2015/50-percent-Chance-of-Awesome/part2_exploratory_analysis/")
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
dataset$WINNER = as.factor(dataset$WINNER)
train = sample(1:nrow(dataset),nrow(dataset)*2/3)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")])
#check the performance of the model using test data
pred = predict(tree_model, testdata, type = "class")
length(pred)
print(paste0("accuracy: ", mean(pred==testdata$WINNER)),quote = FALSE)
cv_tree = cv.tree(tree_model, FUN = prune.tree)
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type = "b")
prune_model = prune.tree(tree_model, best = 3)
predict_after_prune = predict(prune_model, testdata)
View(predict_after_prune)
predict_after_prune = predict(prune_model, testdata, type = "class")
confmatrix = (table(predict_after_prune, testdata$WINNER))
print("confusion matrix",quote = FALSE)
print(confmatrix)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(predict_after_prune)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
jpeg(paste0("hypothesis_testing/data_driving_predictive_models/DT ROC ", k, ".jpeg", sep = ""))
plot(myROC)
dev.off()
print(paste0("accuracy: ", mean(predict_after_prune==testdata$WINNER)),quote = FALSE)
print(paste0("precision: ", precision),quote = FALSE)
source('~/Documents/Analytics 501 Fall 2015/50-percent-Chance-of-Awesome/part2_exploratory_analysis/hypothesis_testing/data_driving_predictive_models/DT.R', echo=TRUE)
table(c(1,2), c(3,4))
source("hypothesis_testing/data_driving_predictive_models/DT.R")
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
dataset = dataset[, mapply(is.numeric, dataset)]
dataset = dataset[, -c(2,3, 12,13,14,15,10,9,7)]
names(dataset)
setwd("~/Documents/Analytics 501 Fall 2015/50-percent-Chance-of-Awesome/part2_exploratory_analysis/")
political_data = read.csv("PoldataSPIndustriesStockData no outliers.csv")
# Null Hyothesis: The Amount of Money and the Number of Industries backinf a candidate do not affect
# the chances of the candidate winning said election.
political_data = political_data[order(political_data$INDRANK, decreasing = T),]
Logit_pol_data = political_data[!duplicated(political_data[,c("STATE", "DISTRICT", "CANDIDATE", "YEAR")]),]
Logit_pol_data$WINNER = as.factor(Logit_pol_data$WINNER)
model <- glm(WINNER ~ CANDTOTAL + INDRANK,data=Logit_pol_data, family = "binomial")
summary(model)
estimatedwinners = predict(model, type = c("class"))
estimatedwinners = predict(model,Logit_pol_data, type = c("class"))
estimatedwinners = predict(model,Logit_pol_data)
mean(estimatedwinners == Logit_pol_data$WINNER)
estimatedwinners
setwd("~/Documents/Analytics 501 Fall 2015/50-percent-Chance-of-Awesome/part2_exploratory_analysis/")
political_data = read.csv("PoldataSPIndustriesStockData no outliers.csv")
# Null Hyothesis: The Amount of Money and the Number of Industries backinf a candidate do not affect
# the chances of the candidate winning said election.
political_data = political_data[order(political_data$INDRANK, decreasing = T),]
Logit_pol_data = political_data[!duplicated(political_data[,c("STATE", "DISTRICT", "CANDIDATE", "YEAR")]),]
model <- glm(WINNER ~ CANDTOTAL + INDRANK,data=Logit_pol_data, family = "binomial")
summary(model)
# Call:
#   glm(formula = WINNER ~ CANDTOTAL + INDRANK, family = "binomial",
#       data = Logit_pol_data)
#
# Deviance Residuals:
#   Min       1Q   Median       3Q      Max
# -3.1271  -0.6817   0.2709   0.7789   2.1813
#
# Coefficients:
#   Estimate Std. Error z value Pr(>|z|)
# (Intercept) -4.711e+00  2.132e-01  -22.09   <2e-16 ***
#   CANDTOTAL    3.405e-06  1.295e-07   26.30   <2e-16 ***
#   INDRANK      4.718e-01  2.704e-02   17.45   <2e-16 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
#
# (Dispersion parameter for binomial family taken to be 1)
#
# Null deviance: 5057.5  on 3668  degrees of freedom
# Residual deviance: 3351.9  on 3666  degrees of freedom
# AIC: 3357.9
#
# Number of Fisher Scoring iterations: 5
# Number of Fisher Scoring iterations: 5
# The p-values for all three attributes is less than 0.05 and the overall F-Statistic P-value is
# less than 0.05. Therefore we we reject the null hypothesis that none of these attributes affect
# the winning outcome of a Candidate in favor of the alternative hypthesis.
# In order to figure out ROC curves
# http://stackoverflow.com/questions/18449013/r-logistic-regression-area-under-curver
estimatedwinners = predict(model, type = c("response"))
estimatedwinners[estimatedwinners<.5] = 0
estimatedwinners[estimatedwinners>=.5] = 1
Logit_pol_data$prob = estimatedwinners
mean(Logit_pol_data$prob == Logit_pol_data$WINNER)
source("hypothesis_testing/data_driving_predictive_models/DT.R")
library(ISLR)
library(tree)
#load the dataset
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
dataset$WINNER = as.factor(dataset$WINNER)
dataset = dataset[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")]
source("hypothesis_testing/data_driving_predictive_models/DT.R")
library(ISLR)
library(tree)
#load the dataset
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
dataset$WINNER = as.factor(dataset$WINNER)
dataset = dataset[,c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")]
k = 5
for(k in 1:5){
train = sample(1:nrow(dataset),nrow(dataset)*(k-1)/k)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata)
#check the performance of the model using test data
pred = predict(tree_model, testdata, type = "class")
#using .5 as a standard split
print(paste0("accuracy: ", mean(pred==testdata$WINNER)),quote = FALSE)
#confint(tree_predict==testdata$class)
#how to get mean, variance, confidence interval???????????
#prune the tree to improve the performance
cv_tree = cv.tree(tree_model, FUN = prune.tree)
#see the cv_tree's attributes
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type = "b")
#we can see that when size = 3, the error is least
prune_model = prune.tree(tree_model, best = 3)
#show the tree structure after pruning
jpeg(paste0("hypothesis_testing/data_driving_predictive_models/DT Prune ", k, ".jpeg", sep = ""))
plot(prune_model)
text(prune_model, pretty = 0)
dev.off()
predict_after_prune = predict(prune_model, testdata, type = "class")
confmatrix = (table(predict_after_prune, testdata$WINNER))
print("confusion matrix",quote = FALSE)
print(confmatrix)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(predict_after_prune)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
jpeg(paste0("hypothesis_testing/data_driving_predictive_models/DT ROC ", k, ".jpeg", sep = ""))
plot(myROC)
dev.off()
precision = confmatrix[2,2]/sum(confmatrix[2,2],confmatrix[1,2])
Recall = confmatrix[2,2]/sum(confmatrix[2,2],confmatrix[2,1])
print(paste0("accuracy: ", mean(predict_after_prune==testdata$WINNER)),quote = FALSE)
print(paste0("precision: ", precision),quote = FALSE)
print(paste0("Recall: ", Recall),quote = FALSE)
print(paste0("F-measure: ", 2*precision*Recall/(precision+Recall)),quote = FALSE)
}
train = sample(1:nrow(dataset),nrow(dataset)*(k-1)/k)
nrow(dataset)
nrow(dataset)*(k-1)/k
k = 5
nrow(dataset)*(k-1)/k
source("hypothesis_testing/data_driving_predictive_models/DT.R")
source("hypothesis_testing/data_driving_predictive_models/kNN.R")
library(class)
dataset = read.csv("PoldataSPIndustriesStockData no outliers.csv")
dataset = dataset[order(dataset$INDRANK, decreasing = T),]
dataset = dataset[!duplicated(dataset[,c("YEAR", "STATE", "DISTRICT", "CANDIDATE")]),]
dataset = dataset[, mapply(is.numeric, dataset)]
dataset = dataset[, c("CANDTOTAL", "INCUMBENT", "WINNER", "INDRANK")]
train = sample(1:nrow(dataset),nrow(dataset)*(k-1)/k)
test = -train
traindataset = dataset[train,]
testdataset = dataset[test,]
trainTarget = traindataset[, 5]
source("hypothesis_testing/data_driving_predictive_models/kNN.R")
source("hypothesis_testing/data_driving_predictive_models/kNN.R")
source("hypothesis_testing/data_driving_predictive_models/kNN.R")
source("hypothesis_testing/data_driving_predictive_models/NaiveBayes.R")
source("hypothesis_testing/data_driving_predictive_models/NaiveBayes.R")
source("hypothesis_testing/data_driving_predictive_models/kNN.R")
source("hypothesis_testing/data_driving_predictive_models/DT.R")
source("hypothesis_testing/data_driving_predictive_models/DT.R")
