#load the dataset
dataset = read.csv("data.csv")
"
#bin the data
binOfIndustryPercent = 1/6
binOfCandTotal = max(dataset$CANDTOTAL)/10
binOfAmount = max(dataset$AMOUNT.of.1)/10
x = dataset[, 2]
cut(x, b=6)
industryPercentLevel = cut(x, b=6, labels=c(1:6))
dataset = cbind(dataset, industryPercentLevel)
x = dataset[, 3]
candTotalLevel = cut(x, b=10, labels = c(1:10))
dataset = cbind(dataset, candTotalLevel)
a1 = dataset[, 11]
amount1Level = cut(a1, b=10, labels = c(1:10))
dataset = cbind(dataset, amount1Level)
a2 = dataset[, 11]
amount2Level = cut(a2, b=10, labels = c(1:10))
dataset = cbind(dataset, amount2Level)
a3 = dataset[, 11]
amount3Level = cut(a3, b=10, labels = c(1:10))
dataset = cbind(dataset, amount3Level)
a4 = dataset[, 11]
amount4Level = cut(a4, b=10, labels = c(1:10))
dataset = cbind(dataset, amount4Level)
a5 = dataset[, 11]
amount5Level = cut(a5, b=10, labels = c(1:10))
dataset = cbind(dataset, amount5Level)
dataset = dataset[, c(1,4:10,16:22)]
"
#mons = factor(mons,levels=c("Not for profit","Financials","Consumer Staples","Industrials","Consumer Discretionary","Materials","Utilities","Information Technology","Not publicly traded","Health Care","Telecommunication Services","Energy","Other"),ordered=TRUE)
#take a look at first several rows
head(dataset)
#range(dataset$attr1)
#append a new column named High
#High = ifelse(trainset$attr1>6.2,"Yes","No")
#trainset = data.frame(trainset,High)
#discard one column
#trainset = trainset[2:30]
#just keep one column of the table
#trainset = trainset[2]
#split the data into trainset and testset
#set seed to make sure next time you run the program, it gives you the same result
#10 folds across - get train data and test data
train = sample(1:nrow(dataset),nrow(dataset)*9/10)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#head(trainset)
#head(testset)
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata)
#show the tree structure
plot(tree_model)
text(tree_model, pretty = 0)
#check the performance of the model using test data
tree_predict = predict(tree_model, testdata, type="class")
table(tree_predict, testdata$WINNER)
mean(tree_predict==testdata$WINNER)
var(tree_predict==testdata$WINNER)
#confint(tree_predict==testdata$class)
#how to get mean, variance, confidence interval???????????
#prune the tree to improve the performance
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
#see the cv_tree's attributes
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type = "b")
#we can see that when size==3, the error is least
prune_model = prune.misclass(tree_model, best = 5)
#show the tree structure after pruning
plot(prune_model)
text(prune_model, pretty = 0)
tree_predict_after_prune = predict(prune_model, testdata, type = "class")
table(tree_predict_after_prune, testdata$WINNER)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(tree_predict_after_prune)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
plot(myROC)
mean(tree_predict_after_prune == testdata$WINNER)
var(tree_predict==testdata$WINNER)
library(ISLR)
library(tree)
#load the dataset
dataset = read.csv("data.csv")
"
#bin the data
binOfIndustryPercent = 1/6
binOfCandTotal = max(dataset$CANDTOTAL)/10
binOfAmount = max(dataset$AMOUNT.of.1)/10
x = dataset[, 2]
cut(x, b=6)
industryPercentLevel = cut(x, b=6, labels=c(1:6))
dataset = cbind(dataset, industryPercentLevel)
x = dataset[, 3]
candTotalLevel = cut(x, b=10, labels = c(1:10))
dataset = cbind(dataset, candTotalLevel)
a1 = dataset[, 11]
amount1Level = cut(a1, b=10, labels = c(1:10))
dataset = cbind(dataset, amount1Level)
a2 = dataset[, 11]
amount2Level = cut(a2, b=10, labels = c(1:10))
dataset = cbind(dataset, amount2Level)
a3 = dataset[, 11]
amount3Level = cut(a3, b=10, labels = c(1:10))
dataset = cbind(dataset, amount3Level)
a4 = dataset[, 11]
amount4Level = cut(a4, b=10, labels = c(1:10))
dataset = cbind(dataset, amount4Level)
a5 = dataset[, 11]
amount5Level = cut(a5, b=10, labels = c(1:10))
dataset = cbind(dataset, amount5Level)
dataset = dataset[, c(1,4:10,16:22)]
"
#mons = factor(mons,levels=c("Not for profit","Financials","Consumer Staples","Industrials","Consumer Discretionary","Materials","Utilities","Information Technology","Not publicly traded","Health Care","Telecommunication Services","Energy","Other"),ordered=TRUE)
#take a look at first several rows
head(dataset)
#range(dataset$attr1)
#append a new column named High
#High = ifelse(trainset$attr1>6.2,"Yes","No")
#trainset = data.frame(trainset,High)
#discard one column
#trainset = trainset[2:30]
#just keep one column of the table
#trainset = trainset[2]
#split the data into trainset and testset
#set seed to make sure next time you run the program, it gives you the same result
#10 folds across - get train data and test data
train = sample(1:nrow(dataset),nrow(dataset)*4/5)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#head(trainset)
#head(testset)
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata)
#show the tree structure
plot(tree_model)
text(tree_model, pretty = 0)
#check the performance of the model using test data
tree_predict = predict(tree_model, testdata, type="class")
table(tree_predict, testdata$WINNER)
mean(tree_predict==testdata$WINNER)
var(tree_predict==testdata$WINNER)
#confint(tree_predict==testdata$class)
#how to get mean, variance, confidence interval???????????
#prune the tree to improve the performance
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
#see the cv_tree's attributes
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type = "b")
#we can see that when size==3, the error is least
prune_model = prune.misclass(tree_model, best = 5)
#show the tree structure after pruning
plot(prune_model)
text(prune_model, pretty = 0)
tree_predict_after_prune = predict(prune_model, testdata, type = "class")
table(tree_predict_after_prune, testdata$WINNER)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(tree_predict_after_prune)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
plot(myROC)
mean(tree_predict_after_prune == testdata$WINNER)
var(tree_predict==testdata$WINNER)
myROC = roc(testTarget,result, direction="<", smooth = TRUE, auc=TRUE, ci=TRUE)
plot(myROC)
library(ISLR)
library(tree)
#load the dataset
dataset = read.csv("data.csv")
"
#bin the data
binOfIndustryPercent = 1/6
binOfCandTotal = max(dataset$CANDTOTAL)/10
binOfAmount = max(dataset$AMOUNT.of.1)/10
x = dataset[, 2]
cut(x, b=6)
industryPercentLevel = cut(x, b=6, labels=c(1:6))
dataset = cbind(dataset, industryPercentLevel)
x = dataset[, 3]
candTotalLevel = cut(x, b=10, labels = c(1:10))
dataset = cbind(dataset, candTotalLevel)
a1 = dataset[, 11]
amount1Level = cut(a1, b=10, labels = c(1:10))
dataset = cbind(dataset, amount1Level)
a2 = dataset[, 11]
amount2Level = cut(a2, b=10, labels = c(1:10))
dataset = cbind(dataset, amount2Level)
a3 = dataset[, 11]
amount3Level = cut(a3, b=10, labels = c(1:10))
dataset = cbind(dataset, amount3Level)
a4 = dataset[, 11]
amount4Level = cut(a4, b=10, labels = c(1:10))
dataset = cbind(dataset, amount4Level)
a5 = dataset[, 11]
amount5Level = cut(a5, b=10, labels = c(1:10))
dataset = cbind(dataset, amount5Level)
dataset = dataset[, c(1,4:10,16:22)]
"
#mons = factor(mons,levels=c("Not for profit","Financials","Consumer Staples","Industrials","Consumer Discretionary","Materials","Utilities","Information Technology","Not publicly traded","Health Care","Telecommunication Services","Energy","Other"),ordered=TRUE)
#take a look at first several rows
head(dataset)
#range(dataset$attr1)
#append a new column named High
#High = ifelse(trainset$attr1>6.2,"Yes","No")
#trainset = data.frame(trainset,High)
#discard one column
#trainset = trainset[2:30]
#just keep one column of the table
#trainset = trainset[2]
#split the data into trainset and testset
#set seed to make sure next time you run the program, it gives you the same result
#10 folds across - get train data and test data
train = sample(1:nrow(dataset),nrow(dataset)*9/10)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#head(trainset)
#head(testset)
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata)
#show the tree structure
plot(tree_model)
text(tree_model, pretty = 0)
#check the performance of the model using test data
tree_predict = predict(tree_model, testdata, type="class")
table(tree_predict, testdata$WINNER)
mean(tree_predict==testdata$WINNER)
var(tree_predict==testdata$WINNER)
#confint(tree_predict==testdata$class)
#how to get mean, variance, confidence interval???????????
#prune the tree to improve the performance
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
#see the cv_tree's attributes
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type = "b")
#we can see that when size==3, the error is least
prune_model = prune.misclass(tree_model, best = 5)
#show the tree structure after pruning
plot(prune_model)
text(prune_model, pretty = 0)
tree_predict_after_prune = predict(prune_model, testdata, type = "class")
table(tree_predict_after_prune, testdata$WINNER)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(tree_predict_after_prune)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
plot(myROC)
mean(tree_predict_after_prune == testdata$WINNER)
var(tree_predict==testdata$WINNER)
plot(myROC)
plot(myROC)
library(ISLR)
library(tree)
#load the dataset
dataset = read.csv("data.csv")
"
#bin the data
binOfIndustryPercent = 1/6
binOfCandTotal = max(dataset$CANDTOTAL)/10
binOfAmount = max(dataset$AMOUNT.of.1)/10
x = dataset[, 2]
cut(x, b=6)
industryPercentLevel = cut(x, b=6, labels=c(1:6))
dataset = cbind(dataset, industryPercentLevel)
x = dataset[, 3]
candTotalLevel = cut(x, b=10, labels = c(1:10))
dataset = cbind(dataset, candTotalLevel)
a1 = dataset[, 11]
amount1Level = cut(a1, b=10, labels = c(1:10))
dataset = cbind(dataset, amount1Level)
a2 = dataset[, 11]
amount2Level = cut(a2, b=10, labels = c(1:10))
dataset = cbind(dataset, amount2Level)
a3 = dataset[, 11]
amount3Level = cut(a3, b=10, labels = c(1:10))
dataset = cbind(dataset, amount3Level)
a4 = dataset[, 11]
amount4Level = cut(a4, b=10, labels = c(1:10))
dataset = cbind(dataset, amount4Level)
a5 = dataset[, 11]
amount5Level = cut(a5, b=10, labels = c(1:10))
dataset = cbind(dataset, amount5Level)
dataset = dataset[, c(1,4:10,16:22)]
"
#mons = factor(mons,levels=c("Not for profit","Financials","Consumer Staples","Industrials","Consumer Discretionary","Materials","Utilities","Information Technology","Not publicly traded","Health Care","Telecommunication Services","Energy","Other"),ordered=TRUE)
#take a look at first several rows
head(dataset)
#range(dataset$attr1)
#append a new column named High
#High = ifelse(trainset$attr1>6.2,"Yes","No")
#trainset = data.frame(trainset,High)
#discard one column
#trainset = trainset[2:30]
#just keep one column of the table
#trainset = trainset[2]
#split the data into trainset and testset
#set seed to make sure next time you run the program, it gives you the same result
#10 folds across - get train data and test data
train = sample(1:nrow(dataset),nrow(dataset)*9/10)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#head(trainset)
#head(testset)
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata)
#show the tree structure
plot(tree_model)
text(tree_model, pretty = 0)
#check the performance of the model using test data
tree_predict = predict(tree_model, testdata, type="class")
table(tree_predict, testdata$WINNER)
mean(tree_predict==testdata$WINNER)
var(tree_predict==testdata$WINNER)
#confint(tree_predict==testdata$class)
#how to get mean, variance, confidence interval???????????
#prune the tree to improve the performance
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
#see the cv_tree's attributes
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type = "b")
#we can see that when size==3, the error is least
prune_model = prune.misclass(tree_model, best = 5)
#show the tree structure after pruning
plot(prune_model)
text(prune_model, pretty = 0)
tree_predict_after_prune = predict(prune_model, testdata, type = "class")
table(tree_predict_after_prune, testdata$WINNER)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(tree_predict_after_prune)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
plot(myROC)
mean(tree_predict_after_prune == testdata$WINNER)
var(tree_predict==testdata$WINNER)
library(class)
#load the dataset
dataset = read.csv("./kNNData.csv")
normalize<-function(x){
return ((x-min(x))/(max(x)-min(x)))
}
normalize(c(1,2,3,4,5))
str(dataset)
normalizedDataset = as.data.frame(lapply(dataset[,c(1:4,6:15)], normalize))
str(normalizedDataset)
summary(normalizedDataset)
preprocDataset = cbind(dataset[,1], normalizedDataset)
#10-folds-accross
train = sample(1:nrow(dataset),nrow(dataset)*9/10)
test = -train
traindataset = dataset[train,]
testdataset = dataset[test,]
#10-folds-accross
#train = sample(1:nrow(normalizedDataset),nrow(normalizedDataset)*9/10)
#test = -train
traindata = traindataset[,c(1:4,6:15)]
testdata = testdataset[,c(1:4,6:15)]
trainTarget = traindataset[, 5]
testTarget = testdataset[, 5]
#choose sqrt(569) as the k
sqrt(569)
result = knn(train = traindata, test = testdata, cl = trainTarget, k = 24)
table(testTarget, result)
library(pROC)
testTarget = as.numeric(testTarget)
result = as.numeric(result)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
plot(myROC)
mean(testTarget==result)
var(testTarget==result)
library(e1071)
#load the dataset
dataset = read.csv("./data.csv")
#attach(Smarket)
#bin the data
x = dataset[, 2]
industryPercentLevel = cut(x, b=6, labels=c(1:6))
dataset = cbind(dataset, industryPercentLevel)
x = dataset[, 3]
candTotalLevel = cut(x, b=10, labels = c(1:10))
dataset = cbind(dataset, candTotalLevel)
a1 = dataset[, 11]
amount1Level = cut(a1, b=10, labels = c(1:10))
dataset = cbind(dataset, amount1Level)
a2 = dataset[, 11]
amount2Level = cut(a2, b=10, labels = c(1:10))
dataset = cbind(dataset, amount2Level)
a3 = dataset[, 11]
amount3Level = cut(a3, b=10, labels = c(1:10))
dataset = cbind(dataset, amount3Level)
a4 = dataset[, 11]
amount4Level = cut(a4, b=10, labels = c(1:10))
dataset = cbind(dataset, amount4Level)
a5 = dataset[, 11]
amount5Level = cut(a5, b=10, labels = c(1:10))
dataset = cbind(dataset, amount5Level)
dataset = dataset[, c(1,4:10,16:22)]
#dataset = dataset[,c(1,3:15)]
#10-folds-accross
train = sample(1:nrow(dataset),nrow(dataset)*9/10)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#traindata$WINNER is the label.
nb = naiveBayes(traindata$WINNER~., data=traindata)
#use naive bayes to do prediction
pred = predict(nb, testdata,type =c("class"))
#show the result
table(pred, testdata$WINNER)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(pred)
myROC = roc(testTarget,result,
print.auc=TRUE,
main="Confidence intervals", percent=TRUE,
ci=TRUE,  auc.polygon=TRUE, grid=TRUE, plot=FALSE)
plot(myROC)
#the performance
mean(pred==testdata$WINNER)
var(pred==testdata$WINNER)
library(ISLR)
library(tree)
#load the dataset
dataset = read.csv("data.csv")
"
#bin the data
binOfIndustryPercent = 1/6
binOfCandTotal = max(dataset$CANDTOTAL)/10
binOfAmount = max(dataset$AMOUNT.of.1)/10
x = dataset[, 2]
cut(x, b=6)
industryPercentLevel = cut(x, b=6, labels=c(1:6))
dataset = cbind(dataset, industryPercentLevel)
x = dataset[, 3]
candTotalLevel = cut(x, b=10, labels = c(1:10))
dataset = cbind(dataset, candTotalLevel)
a1 = dataset[, 11]
amount1Level = cut(a1, b=10, labels = c(1:10))
dataset = cbind(dataset, amount1Level)
a2 = dataset[, 11]
amount2Level = cut(a2, b=10, labels = c(1:10))
dataset = cbind(dataset, amount2Level)
a3 = dataset[, 11]
amount3Level = cut(a3, b=10, labels = c(1:10))
dataset = cbind(dataset, amount3Level)
a4 = dataset[, 11]
amount4Level = cut(a4, b=10, labels = c(1:10))
dataset = cbind(dataset, amount4Level)
a5 = dataset[, 11]
amount5Level = cut(a5, b=10, labels = c(1:10))
dataset = cbind(dataset, amount5Level)
dataset = dataset[, c(1,4:10,16:22)]
"
#mons = factor(mons,levels=c("Not for profit","Financials","Consumer Staples","Industrials","Consumer Discretionary","Materials","Utilities","Information Technology","Not publicly traded","Health Care","Telecommunication Services","Energy","Other"),ordered=TRUE)
#take a look at first several rows
head(dataset)
#range(dataset$attr1)
#append a new column named High
#High = ifelse(trainset$attr1>6.2,"Yes","No")
#trainset = data.frame(trainset,High)
#discard one column
#trainset = trainset[2:30]
#just keep one column of the table
#trainset = trainset[2]
#split the data into trainset and testset
#set seed to make sure next time you run the program, it gives you the same result
#10 folds across - get train data and test data
train = sample(1:nrow(dataset),nrow(dataset)*9/10)
test = -train
traindata = dataset[train,]
testdata = dataset[test,]
#head(trainset)
#head(testset)
#build the tree model using the train data
#we want to predict the attribute class, we use all other attributes to train this model, the dataset we use is traindata
tree_model = tree(traindata$WINNER~.,traindata)
#show the tree structure
plot(tree_model)
text(tree_model, pretty = 0)
#check the performance of the model using test data
tree_predict = predict(tree_model, testdata, type="class")
table(tree_predict, testdata$WINNER)
mean(tree_predict==testdata$WINNER)
var(tree_predict==testdata$WINNER)
#confint(tree_predict==testdata$class)
#how to get mean, variance, confidence interval???????????
#prune the tree to improve the performance
cv_tree = cv.tree(tree_model, FUN = prune.misclass)
#see the cv_tree's attributes
names(cv_tree)
plot(cv_tree$size, cv_tree$dev, type = "b")
#we can see that when size==3, the error is least
prune_model = prune.misclass(tree_model, best = 5)
#show the tree structure after pruning
plot(prune_model)
text(prune_model, pretty = 0)
tree_predict_after_prune = predict(prune_model, testdata, type = "class")
table(tree_predict_after_prune, testdata$WINNER)
library(pROC)
testTarget = as.numeric(testdata$WINNER)
result = as.numeric(tree_predict_after_prune)
myROC = roc(testTarget,result, direction="<", auc=TRUE, ci=TRUE)
plot(myROC)
mean(tree_predict_after_prune == testdata$WINNER)
var(tree_predict==testdata$WINNER)
